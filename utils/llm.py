"""
llm.py ‚Äî Wrapper de comunica√ß√£o com o modelo Gemini (Google Generative AI)
para o projeto CALIA V2.0.

Respons√°vel por:
 - Inicializar o cliente da API Gemini usando a chave armazenada em .env
 - Enviar prompts de texto e retornar as respostas geradas
 - Fazer tratamento de erros e manter um fallback de seguran√ßa

Autor: Carlos
Projeto: CALIA V2.0
"""

import os
import logging
from dotenv import load_dotenv
import google.generativeai as genai

# Configura√ß√£o do logger local
logger = logging.getLogger(__name__)


# =====================================
# ‚öôÔ∏è Inicializa√ß√£o da API Gemini
# =====================================
class LLMWrapper:
    """
    Classe de alto n√≠vel para intera√ß√£o com o modelo Gemini da Google.

    Essa classe centraliza toda a comunica√ß√£o com o modelo de linguagem,
    permitindo f√°cil substitui√ß√£o ou troca de modelo no futuro (por ex: GPT, Claude, Mistral, etc.).

    Uso:
        llm = LLMWrapper()
        resposta = llm.send("Explique o conceito de RAG.")
    """

    def __init__(self, api_key_env: str = "API_KEY_CALIA", model_name: str = "gemini-2.5-flash"):
        """
        Inicializa a classe, configurando o cliente da API Gemini.

        Args:
            api_key_env (str): Nome da vari√°vel de ambiente onde a API key est√° armazenada.
                               Por padr√£o: 'API_KEY_CALIA'
            model_name (str): Nome do modelo Gemini a ser usado.
                              (ex: 'gemini-1.5-flash', 'gemini-1.5-pro', etc.)
        """
        load_dotenv()

        self.api_key = os.getenv(api_key_env)
        self.model_name = model_name
        self.model = None

        if not self.api_key:
            logger.warning(f"‚ö†Ô∏è Chave de API '{api_key_env}' n√£o encontrada no .env.")
        else:
            try:
                genai.configure(api_key=self.api_key)
                self.model = genai.GenerativeModel(self.model_name)
                logger.info(f"ü§ñ Modelo Gemini configurado: {self.model_name}")
            except Exception as e:
                logger.exception(f"‚ùå Erro ao inicializar o modelo Gemini: {e}")

    # =====================================
    # üß† Envio de prompt e gera√ß√£o de texto
    # =====================================
    def send(self, prompt: str) -> str:
        """
        Envia um prompt de texto ao modelo Gemini e retorna a resposta.

        Args:
            prompt (str): Texto ou comando a ser processado pelo modelo.

        Returns:
            str: Resposta textual gerada pelo modelo.
        """
        if not self.model:
            logger.warning("‚ö†Ô∏è Modelo LLM n√£o configurado corretamente.")
            return "[ERRO] Modelo n√£o configurado. Verifique a API_KEY_CALIA no .env."

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip() if hasattr(response, "text") else str(response)
            logger.info("üí¨ Resposta recebida do Gemini com sucesso.")
            return text
        except Exception as e:
            logger.error(f"‚ùå Erro ao enviar prompt para o Gemini: {e}")
            return "[ERRO] Falha ao gerar resposta com o modelo Gemini."


# =====================================
# üß™ Teste r√°pido
# =====================================
if __name__ == "__main__":
    """
    Teste direto do m√≥dulo LLMWrapper.
    Permite verificar se a API Gemini est√° funcionando corretamente.
    """
    llm = LLMWrapper()
    resposta = llm.send("Ol√°! Voc√™ est√° funcionando?")
    print("Resposta:", resposta)
